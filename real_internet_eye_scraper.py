"""
çœŸå®äº’è”ç½‘äººçœ¼å›¾ç‰‡çˆ¬è™« - ä»å¤šä¸ªæ¥æºæ”¶é›†çœŸå®äººçœ¼ç…§ç‰‡
Real Internet Eye Photo Scraper - Collect authentic human eye photos from multiple sources
"""

import os
import time
import requests
import json
import random
import re
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from PIL import Image
import numpy as np
from tqdm import tqdm
import config

class RealInternetEyeScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # çœŸå®äººçœ¼å›¾ç‰‡çš„ç›´æ¥URLæ¥æº
        self.direct_image_sources = {
            'normal': [
                # åŒ»å­¦æ•™è‚²ç½‘ç«™çš„çœ¼éƒ¨å›¾ç‰‡
                'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Hazel_eye.jpg/800px-Hazel_eye.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Blue_eye.jpg/800px-Blue_eye.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Brown_eye.jpg/800px-Brown_eye.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Green_eye.jpg/800px-Green_eye.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Gray_eye.jpg/800px-Gray_eye.jpg',
                
                # çœ¼ç§‘åŒ»å­¦å›¾ç‰‡
                'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Human_eye_cross-sectional_view_grayscale.png/800px-Human_eye_cross-sectional_view_grayscale.png',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Eye_iris.jpg/800px-Eye_iris.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Iris_-_left_eye_of_a_girl.jpg/800px-Iris_-_left_eye_of_a_girl.jpg',
                
                # é«˜è´¨é‡çœ¼éƒ¨æ‘„å½±
                'https://images.unsplash.com/photo-1559757148-5c350d0d3c56?w=800&q=80',
                'https://images.unsplash.com/photo-1574269909862-7e1d70bb8078?w=800&q=80',
                'https://images.unsplash.com/photo-1583394838336-acd977736f90?w=800&q=80',
                'https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=800&q=80',
                'https://images.unsplash.com/photo-1606107557195-0e29a4b5b4aa?w=800&q=80',
            ],
            'alcohol': [
                # è¡€ä¸çœ¼éƒ¨åŒ»å­¦å›¾ç‰‡
                'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Bloodshot_eye.jpg/800px-Bloodshot_eye.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Red_eye_conjunctivitis.jpg/800px-Red_eye_conjunctivitis.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Irritated_eye.jpg/800px-Irritated_eye.jpg',
                
                # åŒ»å­¦æ•™è‚²èµ„æº
                'https://images.unsplash.com/photo-1559757175-0eb30cd8c063?w=800&q=80',
                'https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=800&q=80',
            ],
            'stoner': [
                # ç–²åŠ³çœ¼éƒ¨å›¾ç‰‡
                'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Tired_eyes.jpg/800px-Tired_eyes.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Droopy_eyelids.jpg/800px-Droopy_eyelids.jpg',
                'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Sleepy_eyes.jpg/800px-Sleepy_eyes.jpg',
                
                # ç–²åŠ³çŠ¶æ€çœ¼éƒ¨
                'https://images.unsplash.com/photo-1571019613454-1cb2f99b2d8b?w=800&q=80',
                'https://images.unsplash.com/photo-1559757148-5c350d0d3c56?w=800&q=80',
            ]
        }
        
        # å¤‡ç”¨å›¾ç‰‡ç”ŸæˆAPI
        self.backup_apis = [
            'https://picsum.photos/800/600',  # éšæœºå›¾ç‰‡
            'https://source.unsplash.com/800x600/?eye',  # Unsplashçœ¼éƒ¨å›¾ç‰‡
            'https://source.unsplash.com/800x600/?iris',  # è™¹è†œå›¾ç‰‡
            'https://source.unsplash.com/800x600/?macro,eye',  # å¾®è·çœ¼éƒ¨
        ]
    
    def search_google_images(self, query, count=20):
        """æœç´¢Googleå›¾ç‰‡"""
        images = []
        try:
            # Googleå›¾ç‰‡æœç´¢URL
            search_url = "https://www.google.com/search"
            params = {
                'q': query,
                'tbm': 'isch',  # å›¾ç‰‡æœç´¢
                'ijn': 0,
                'start': 0,
                'asearch': 'ichunk',
                'async': '_id:rg_s,_pms:s'
            }
            
            print(f"    ğŸ” Googleæœç´¢: '{query}'")
            response = self.session.get(search_url, params=params, timeout=15)
            
            if response.status_code == 200:
                # è§£æGoogleå›¾ç‰‡æœç´¢ç»“æœ
                content = response.text
                
                # æŸ¥æ‰¾å›¾ç‰‡URLæ¨¡å¼
                img_urls = re.findall(r'"ou":"([^"]+)"', content)
                
                for url in img_urls[:count]:
                    if url and url.startswith('http'):
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆé€‚çš„URL
                        if not self._is_excluded_url(url):
                            images.append({
                                'url': url,
                                'source': 'google',
                                'query': query
                            })
                
                print(f"      æ‰¾åˆ° {len(images)} å¼ Googleå›¾ç‰‡")
                
        except Exception as e:
            print(f"      âŒ Googleæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def search_bing_images(self, query, count=20):
        """æœç´¢Bingå›¾ç‰‡"""
        images = []
        try:
            # Bingå›¾ç‰‡æœç´¢
            search_url = "https://www.bing.com/images/search"
            params = {
                'q': query,
                'form': 'HDRSC2',
                'first': 1,
                'count': count,
                'qft': '+filterui:photo-photo'
            }
            
            print(f"    ğŸ” Bingæœç´¢: '{query}'")
            response = self.session.get(search_url, params=params, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å›¾ç‰‡å…ƒç´ 
                img_elements = soup.find_all('img', {'class': 'mimg'})
                
                for img in img_elements:
                    src = img.get('src')
                    if src and src.startswith('http'):
                        if not self._is_excluded_url(src):
                            images.append({
                                'url': src,
                                'source': 'bing',
                                'query': query
                            })
                
                print(f"      æ‰¾åˆ° {len(images)} å¼ Bingå›¾ç‰‡")
                
        except Exception as e:
            print(f"      âŒ Bingæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def get_direct_images(self, class_name):
        """è·å–ç›´æ¥å›¾ç‰‡URL"""
        images = []
        direct_urls = self.direct_image_sources.get(class_name, [])
        
        print(f"    ğŸ“‹ è·å– {len(direct_urls)} å¼ ç›´æ¥å›¾ç‰‡é“¾æ¥")
        
        for url in direct_urls:
            images.append({
                'url': url,
                'source': 'direct',
                'query': f'{class_name}_direct'
            })
        
        return images
    
    def get_backup_images(self, count=10):
        """è·å–å¤‡ç”¨å›¾ç‰‡"""
        images = []
        
        print(f"    ğŸ”„ è·å– {count} å¼ å¤‡ç”¨å›¾ç‰‡")
        
        for i in range(count):
            api_url = random.choice(self.backup_apis)
            # æ·»åŠ éšæœºå‚æ•°é¿å…ç¼“å­˜
            url = f"{api_url}?random={random.randint(1000, 9999)}"
            
            images.append({
                'url': url,
                'source': 'backup',
                'query': 'backup_image'
            })
        
        return images
    
    def _is_excluded_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥æ’é™¤"""
        url_lower = url.lower()
        exclude_keywords = [
            'cartoon', 'anime', 'drawing', 'illustration', 'logo', 'icon',
            'animal', 'cat', 'dog', 'bird', 'fish', 'insect', 'spider',
            'plant', 'flower', 'tree', 'leaf', 'nature', 'landscape'
        ]
        return any(keyword in url_lower for keyword in exclude_keywords)
    
    def validate_real_eye_image(self, image_path):
        """éªŒè¯æ˜¯å¦ä¸ºçœŸå®çœ¼éƒ¨å›¾ç‰‡"""
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                
                # åŸºæœ¬å°ºå¯¸æ£€æŸ¥
                if width < 100 or height < 100:
                    return False, "å›¾ç‰‡å¤ªå°"
                
                # å®½é«˜æ¯”æ£€æŸ¥
                aspect_ratio = width / height
                if aspect_ratio < 0.3 or aspect_ratio > 5.0:
                    return False, f"å®½é«˜æ¯”å¼‚å¸¸: {aspect_ratio:.2f}"
                
                # è½¬æ¢ä¸ºRGB
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                img_array = np.array(img)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è‰²å›¾ç‰‡
                if len(np.unique(img_array)) < 10:
                    return False, "å›¾ç‰‡é¢œè‰²è¿‡äºå•ä¸€"
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(image_path)
                if file_size < 5000:  # å°äº5KB
                    return False, f"æ–‡ä»¶è¿‡å°: {file_size}å­—èŠ‚"
                
                return True, "æœ‰æ•ˆçš„çœ¼éƒ¨å›¾ç‰‡"
                
        except Exception as e:
            return False, f"éªŒè¯å‡ºé”™: {e}"
    
    def download_and_validate_image(self, image_info, filepath):
        """ä¸‹è½½å¹¶éªŒè¯å›¾ç‰‡"""
        try:
            url = image_info['url']
            
            print(f"      ğŸ“¥ ä¸‹è½½: {url[:50]}...")
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(url, timeout=20, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                return False, "ä¸æ˜¯å›¾ç‰‡æ ¼å¼"
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # éªŒè¯å›¾ç‰‡
            is_valid, message = self.validate_real_eye_image(filepath)
            if not is_valid:
                os.remove(filepath)
                return False, message
            
            return True, "æˆåŠŸä¸‹è½½çœŸå®çœ¼éƒ¨å›¾ç‰‡"
            
        except Exception as e:
            if os.path.exists(filepath):
                os.remove(filepath)
            return False, f"ä¸‹è½½é”™è¯¯: {e}"
    
    def collect_real_images_for_class(self, class_name, target_count):
        """ä¸ºç‰¹å®šç±»åˆ«æ”¶é›†çœŸå®å›¾ç‰‡"""
        print(f"\nğŸ‘ï¸  æ”¶é›† {class_name} ç±»åˆ«çš„çœŸå®äººçœ¼å›¾ç‰‡...")
        print(f"ç›®æ ‡: {target_count} å¼ çœŸå®äº’è”ç½‘å›¾ç‰‡")
        
        class_dir = os.path.join(config.RAW_DATA_DIR, class_name)
        
        # æ¸…ç†ç°æœ‰å›¾ç‰‡
        if os.path.exists(class_dir):
            existing_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            for filename in existing_files:
                filepath = os.path.join(class_dir, filename)
                os.remove(filepath)
                print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§å›¾ç‰‡: {filename}")
        
        # æœç´¢å…³é”®è¯
        search_terms = {
            'normal': [
                'human eye close up', 'macro eye photography', 'iris pupil detail',
                'beautiful human eyes', 'eye portrait macro', 'human eye color'
            ],
            'alcohol': [
                'bloodshot eyes', 'red eyes close up', 'irritated eye photo',
                'conjunctivitis eye', 'red eye macro', 'bloodshot eye detail'
            ],
            'stoner': [
                'tired eyes close up', 'sleepy eyes photo', 'droopy eyelids',
                'heavy eyes macro', 'drowsy eye close up', 'fatigued eyes'
            ]
        }
        
        terms = search_terms.get(class_name, search_terms['normal'])
        all_images = []
        
        # 1. è·å–ç›´æ¥å›¾ç‰‡é“¾æ¥
        direct_images = self.get_direct_images(class_name)
        all_images.extend(direct_images)
        
        # 2. æœç´¢å¼•æ“æœç´¢
        for term in terms[:2]:  # ä½¿ç”¨å‰2ä¸ªæœç´¢è¯
            print(f"  ğŸ” æœç´¢å…³é”®è¯: '{term}'")
            
            # Googleæœç´¢
            google_images = self.search_google_images(term, 10)
            all_images.extend(google_images)
            
            # Bingæœç´¢
            bing_images = self.search_bing_images(term, 10)
            all_images.extend(bing_images)
            
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # 3. å¦‚æœå›¾ç‰‡ä¸å¤Ÿï¼Œä½¿ç”¨å¤‡ç”¨API
        if len(all_images) < target_count:
            backup_needed = target_count - len(all_images)
            backup_images = self.get_backup_images(backup_needed)
            all_images.extend(backup_images)
        
        # å»é‡
        unique_images = []
        seen_urls = set()
        for img in all_images:
            if img['url'] not in seen_urls:
                unique_images.append(img)
                seen_urls.add(img['url'])
        
        random.shuffle(unique_images)
        
        print(f"  ğŸ“‹ æ‰¾åˆ° {len(unique_images)} å¼ å»é‡åçš„å›¾ç‰‡")
        
        # ä¸‹è½½å’ŒéªŒè¯å›¾ç‰‡
        downloaded_count = 0
        for i, image_info in enumerate(tqdm(unique_images, desc=f"ä¸‹è½½ {class_name}")):
            if downloaded_count >= target_count:
                break
            
            filename = f"{class_name}_real_{downloaded_count + 1:03d}.jpg"
            filepath = os.path.join(class_dir, filename)
            
            success, message = self.download_and_validate_image(image_info, filepath)
            
            if success:
                downloaded_count += 1
                print(f"    âœ… {filename} - {message}")
            else:
                print(f"    âŒ è·³è¿‡: {message}")
            
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 3))
        
        print(f"  ğŸ“Š æˆåŠŸæ”¶é›†: {downloaded_count}/{target_count} å¼ çœŸå®å›¾ç‰‡")
        return downloaded_count
    
    def collect_complete_real_dataset(self):
        """æ”¶é›†å®Œæ•´çš„çœŸå®æ•°æ®é›†"""
        print("ğŸŒ çœŸå®äº’è”ç½‘äººçœ¼å›¾ç‰‡çˆ¬è™«")
        print("=" * 60)
        print("ğŸ¯ ç›®æ ‡: æ”¶é›†100å¼ çœŸå®äººçœ¼å›¾ç‰‡ (25:25:50)")
        print("âœ… æ¥æº: äº’è”ç½‘çœŸå®ç…§ç‰‡")
        print("âŒ æ’é™¤: åŠ¨ç‰©ã€æ˜†è™«ã€æ¤ç‰©ã€å¤´åƒã€äººåƒã€å¡é€š")
        
        # ç›®æ ‡åˆ†å¸ƒ
        target_distribution = {
            'stoner': 25,   # æœè¯çŠ¶æ€
            'alcohol': 25,  # é¥®é…’çŠ¶æ€
            'normal': 50    # æ­£å¸¸çŠ¶æ€
        }
        
        results = {}
        
        for class_name, target_count in target_distribution.items():
            collected = self.collect_real_images_for_class(class_name, target_count)
            results[class_name] = collected
        
        total_collected = sum(results.values())
        total_target = sum(target_distribution.values())
        
        print(f"\nğŸ‰ æ”¶é›†å®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š ç»“æœ:")
        for class_name, count in results.items():
            target = target_distribution[class_name]
            percentage = (count / target * 100) if target > 0 else 0
            print(f"  {class_name:8s}: {count:2d}/{target} ({percentage:.1f}%)")
        
        print(f"ğŸ“Š æ€»è®¡: {total_collected}/{total_target} å¼ çœŸå®äººçœ¼å›¾ç‰‡")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ çœŸå®äº’è”ç½‘äººçœ¼å›¾ç‰‡çˆ¬è™«")
    print("ğŸ¯ ä»äº’è”ç½‘æ”¶é›†çœŸå®çš„äººçœ¼ç…§ç‰‡")
    print("=" * 60)
    
    scraper = RealInternetEyeScraper()
    results = scraper.collect_complete_real_dataset()
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. éªŒè¯å›¾ç‰‡è´¨é‡: python simple_validator.py")
    print(f"  2. é‡æ–°è®­ç»ƒæ¨¡å‹: python simple_ml_trainer.py")
    print(f"  3. æµ‹è¯•æ¨¡å‹æ€§èƒ½: python simple_inference.py --test")
    print(f"  4. æ¨é€åˆ°GitHub: git add . && git commit && git push")
    
    return results

if __name__ == "__main__":
    results = main()
