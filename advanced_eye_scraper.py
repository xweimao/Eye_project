"""
é«˜çº§äººçœ¼å›¾ç‰‡çˆ¬è™« - ä¸“é—¨æ”¶é›†çœŸå®äººçœ¼ç…§ç‰‡
Advanced Human Eye Scraper - Specialized for real human eye photos
"""

import os
import time
import requests
import json
import random
import re
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from PIL import Image
import numpy as np
from tqdm import tqdm
import config

class AdvancedEyeScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN,zh;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # ä¸“ä¸šæœç´¢å…³é”®è¯
        self.search_keywords = {
            'normal': [
                'human eye close up macro photography',
                'beautiful human eyes macro shot',
                'human iris pupil close up photo',
                'eye macro photography portrait',
                'human eye detail close up',
                'macro human eye photography',
                'close up human eye shot',
                'human eye iris macro',
                'detailed human eye photo',
                'human eye close up portrait'
            ],
            'alcohol': [
                'bloodshot eyes close up photo',
                'red eyes alcohol effect',
                'bloodshot human eyes macro',
                'red irritated eyes close up',
                'alcohol bloodshot eyes photo',
                'red eyes hangover close up',
                'bloodshot eye macro photography',
                'red tired eyes close up',
                'alcohol red eyes photo',
                'bloodshot eye detail shot'
            ],
            'stoner': [
                'droopy sleepy eyes close up',
                'tired heavy eyelids photo',
                'sleepy droopy eyes macro',
                'cannabis droopy eyes photo',
                'heavy eyelids close up shot',
                'sleepy tired eyes macro',
                'droopy eyelids photography',
                'tired droopy eyes close up',
                'sleepy eyes macro photo',
                'heavy droopy eyelids shot'
            ]
        }
        
        # æ’é™¤å…³é”®è¯
        self.exclude_keywords = [
            'animal', 'cat', 'dog', 'bird', 'fish', 'insect', 'spider', 'snake',
            'cartoon', 'anime', 'drawing', 'illustration', 'art', 'painting',
            'diagram', 'medical diagram', 'anatomy chart', 'infographic',
            'logo', 'icon', 'symbol', 'graphic', 'design', 'vector',
            'plant', 'flower', 'leaf', 'tree', 'nature', 'landscape',
            'full face', 'portrait', 'headshot', 'person', 'people', 'man', 'woman'
        ]
    
    def search_bing_images(self, query, count=50):
        """ä½¿ç”¨Bingå›¾ç‰‡æœç´¢"""
        images = []
        try:
            # Bingå›¾ç‰‡æœç´¢URL
            search_url = "https://www.bing.com/images/search"
            params = {
                'q': query,
                'form': 'HDRSC2',
                'first': 1,
                'count': count,
                'qft': '+filterui:photo-photo+filterui:aspect-square+filterui:imagesize-medium'
            }
            
            print(f"    ğŸ” Bingæœç´¢: '{query}'")
            response = self.session.get(search_url, params=params, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
                img_tags = soup.find_all('img', {'class': 'mimg'})
                
                for img_tag in img_tags:
                    src = img_tag.get('src')
                    if src and src.startswith('http'):
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆé€‚çš„å›¾ç‰‡
                        if not self._is_excluded_url(src):
                            images.append({
                                'url': src,
                                'source': 'bing',
                                'query': query,
                                'alt': img_tag.get('alt', '')
                            })
                
                print(f"      æ‰¾åˆ° {len(images)} å¼ æ½œåœ¨å›¾ç‰‡")
                
        except Exception as e:
            print(f"      âŒ Bingæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def search_duckduckgo_images(self, query, count=50):
        """ä½¿ç”¨DuckDuckGoå›¾ç‰‡æœç´¢"""
        images = []
        try:
            # DuckDuckGoå›¾ç‰‡æœç´¢API
            search_url = "https://duckduckgo.com/"
            
            # é¦–å…ˆè·å–æœç´¢token
            response = self.session.get(search_url, timeout=15)
            
            # ç„¶åæœç´¢å›¾ç‰‡
            api_url = "https://duckduckgo.com/i.js"
            params = {
                'l': 'us-en',
                'o': 'json',
                'q': query,
                'vqd': '',  # éœ€è¦ä»é¦–é¡µè·å–
                'f': ',,,',
                'p': '1'
            }
            
            print(f"    ğŸ” DuckDuckGoæœç´¢: '{query}'")
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è§£ævqd token
            
        except Exception as e:
            print(f"      âŒ DuckDuckGoæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def search_unsplash_api(self, query, count=30):
        """ä½¿ç”¨Unsplash APIæœç´¢ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        images = []
        try:
            # æ³¨æ„ï¼šå®é™…ä½¿ç”¨éœ€è¦Unsplash APIå¯†é’¥
            # è¿™é‡Œæä¾›æ¡†æ¶ï¼Œç”¨æˆ·éœ€è¦è‡ªå·±ç”³è¯·APIå¯†é’¥
            api_key = "YOUR_UNSPLASH_API_KEY"  # ç”¨æˆ·éœ€è¦æ›¿æ¢
            
            if api_key == "YOUR_UNSPLASH_API_KEY":
                print(f"      âš ï¸  éœ€è¦Unsplash APIå¯†é’¥")
                return images
            
            api_url = "https://api.unsplash.com/search/photos"
            headers = {'Authorization': f'Client-ID {api_key}'}
            params = {
                'query': query,
                'per_page': count,
                'orientation': 'landscape'
            }
            
            print(f"    ğŸ” Unsplashæœç´¢: '{query}'")
            response = self.session.get(api_url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                for result in data.get('results', []):
                    images.append({
                        'url': result['urls']['regular'],
                        'source': 'unsplash',
                        'query': query,
                        'description': result.get('description', '')
                    })
                
                print(f"      æ‰¾åˆ° {len(images)} å¼ é«˜è´¨é‡å›¾ç‰‡")
                
        except Exception as e:
            print(f"      âŒ Unsplashæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def search_wikimedia_commons(self, query, count=30):
        """æœç´¢Wikimedia Commons"""
        images = []
        try:
            api_url = "https://commons.wikimedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': f'filetype:bitmap {query}',
                'srnamespace': 6,
                'srlimit': count
            }
            
            print(f"    ğŸ” Wikimediaæœç´¢: '{query}'")
            response = self.session.get(api_url, params=params, timeout=15)
            data = response.json()
            
            if 'query' in data and 'search' in data['query']:
                for item in data['query']['search']:
                    title = item['title']
                    if title.startswith('File:'):
                        # è¿‡æ»¤ä¸åˆé€‚çš„æ–‡ä»¶å
                        if not self._is_excluded_filename(title):
                            # è·å–æ–‡ä»¶URL
                            file_params = {
                                'action': 'query',
                                'format': 'json',
                                'titles': title,
                                'prop': 'imageinfo',
                                'iiprop': 'url|size|mime'
                            }
                            
                            file_response = self.session.get(api_url, params=file_params, timeout=10)
                            file_data = file_response.json()
                            
                            if 'query' in file_data and 'pages' in file_data['query']:
                                for page_id, page_data in file_data['query']['pages'].items():
                                    if 'imageinfo' in page_data:
                                        image_info = page_data['imageinfo'][0]
                                        if 'url' in image_info:
                                            mime_type = image_info.get('mime', '')
                                            if mime_type.startswith('image/'):
                                                images.append({
                                                    'url': image_info['url'],
                                                    'source': 'wikimedia',
                                                    'query': query,
                                                    'title': title,
                                                    'size': image_info.get('size', 0)
                                                })
                
                print(f"      æ‰¾åˆ° {len(images)} å¼ Wikimediaå›¾ç‰‡")
                
        except Exception as e:
            print(f"      âŒ Wikimediaæœç´¢å‡ºé”™: {e}")
        
        return images
    
    def _is_excluded_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯"""
        url_lower = url.lower()
        return any(keyword in url_lower for keyword in self.exclude_keywords)
    
    def _is_excluded_filename(self, filename):
        """æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯"""
        filename_lower = filename.lower()
        return any(keyword in filename_lower for keyword in self.exclude_keywords)
    
    def validate_eye_image(self, image_path):
        """éªŒè¯æ˜¯å¦ä¸ºåˆé€‚çš„çœ¼éƒ¨å›¾ç‰‡"""
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                
                # åŸºæœ¬å°ºå¯¸æ£€æŸ¥
                if width < 200 or height < 150:
                    return False, "å›¾ç‰‡å¤ªå°"
                
                # å®½é«˜æ¯”æ£€æŸ¥ï¼ˆçœ¼éƒ¨å›¾ç‰‡é€šå¸¸ä¸ä¼šå¤ªæç«¯ï¼‰
                aspect_ratio = width / height
                if aspect_ratio < 0.5 or aspect_ratio > 3.0:
                    return False, f"å®½é«˜æ¯”ä¸åˆé€‚: {aspect_ratio:.2f}"
                
                # è½¬æ¢ä¸ºRGBè¿›è¡Œåˆ†æ
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                img_array = np.array(img)
                
                # é¢œè‰²åˆ†æ - æ£€æŸ¥æ˜¯å¦æœ‰è‚¤è‰²
                r_channel = img_array[:, :, 0]
                g_channel = img_array[:, :, 1]
                b_channel = img_array[:, :, 2]
                
                # è‚¤è‰²æ£€æµ‹ï¼ˆç®€å•çš„RGBèŒƒå›´ï¼‰
                skin_mask = (
                    (r_channel > 95) & (g_channel > 40) & (b_channel > 20) &
                    (r_channel > g_channel) & (r_channel > b_channel) &
                    (abs(r_channel.astype(int) - g_channel.astype(int)) > 15)
                )
                
                skin_ratio = np.sum(skin_mask) / (width * height)
                if skin_ratio < 0.1:  # è‡³å°‘10%çš„è‚¤è‰²åƒç´ 
                    return False, "ç¼ºå°‘è‚¤è‰²ç‰¹å¾"
                
                # æ£€æŸ¥é¢œè‰²å¤šæ ·æ€§ï¼ˆé¿å…å•è‰²å›¾ç‰‡ï¼‰
                unique_colors = len(np.unique(img_array.reshape(-1, 3), axis=0))
                color_diversity = unique_colors / (width * height)
                if color_diversity < 0.05:
                    return False, "é¢œè‰²è¿‡äºå•ä¸€"
                
                # æ£€æŸ¥æ˜¯å¦è¿‡äºé¥±å’Œï¼ˆå¡é€šç‰¹å¾ï¼‰
                hsv_img = img.convert('HSV')
                hsv_array = np.array(hsv_img)
                saturation = hsv_array[:, :, 1]
                high_saturation_ratio = np.sum(saturation > 200) / (width * height)
                if high_saturation_ratio > 0.4:
                    return False, "é¥±å’Œåº¦è¿‡é«˜ï¼ˆå¯èƒ½æ˜¯å¡é€šï¼‰"
                
                return True, "åˆé€‚çš„çœ¼éƒ¨å›¾ç‰‡"
                
        except Exception as e:
            return False, f"éªŒè¯å‡ºé”™: {e}"
    
    def download_and_validate_image(self, image_info, filepath):
        """ä¸‹è½½å¹¶éªŒè¯å›¾ç‰‡"""
        try:
            url = image_info['url']
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(url, timeout=20, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                return False, "ä¸æ˜¯å›¾ç‰‡æ ¼å¼"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            content_length = response.headers.get('content-length')
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb > 10:  # è¶…è¿‡10MB
                    return False, f"æ–‡ä»¶è¿‡å¤§: {size_mb:.1f}MB"
                if int(content_length) < 10240:  # å°äº10KB
                    return False, f"æ–‡ä»¶è¿‡å°: {int(content_length)/1024:.1f}KB"
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if downloaded > 10 * 1024 * 1024:  # è¶…è¿‡10MBåœæ­¢
                            os.remove(filepath)
                            return False, "ä¸‹è½½è¿‡ç¨‹ä¸­æ–‡ä»¶è¿‡å¤§"
            
            # éªŒè¯å›¾ç‰‡
            is_valid, message = self.validate_eye_image(filepath)
            if not is_valid:
                os.remove(filepath)
                return False, message
            
            return True, "æˆåŠŸä¸‹è½½å¹¶éªŒè¯"
            
        except Exception as e:
            if os.path.exists(filepath):
                os.remove(filepath)
            return False, f"ä¸‹è½½é”™è¯¯: {e}"
    
    def collect_images_for_class(self, class_name, target_count):
        """ä¸ºç‰¹å®šç±»åˆ«æ”¶é›†å›¾ç‰‡"""
        print(f"\nğŸ‘ï¸  æ”¶é›† {class_name} ç±»åˆ«å›¾ç‰‡...")
        print(f"ç›®æ ‡: {target_count} å¼ çœŸå®äººçœ¼å›¾ç‰‡")
        
        class_dir = os.path.join(config.RAW_DATA_DIR, class_name)
        
        # æ¸…ç†ç°æœ‰å›¾ç‰‡
        if os.path.exists(class_dir):
            existing_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            for filename in existing_files:
                filepath = os.path.join(class_dir, filename)
                os.remove(filepath)
                print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§å›¾ç‰‡: {filename}")
        
        keywords = self.search_keywords[class_name]
        all_images = []
        
        # ä»å¤šä¸ªæœç´¢å¼•æ“æ”¶é›†å›¾ç‰‡
        for keyword in keywords[:3]:  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            print(f"  ğŸ” æœç´¢å…³é”®è¯: '{keyword}'")
            
            # Bingæœç´¢
            bing_images = self.search_bing_images(keyword, 20)
            all_images.extend(bing_images)
            
            # Wikimediaæœç´¢
            wiki_images = self.search_wikimedia_commons(keyword, 15)
            all_images.extend(wiki_images)
            
            # Unsplashæœç´¢ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
            unsplash_images = self.search_unsplash_api(keyword, 10)
            all_images.extend(unsplash_images)
            
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # å»é‡å’ŒéšæœºåŒ–
        unique_images = []
        seen_urls = set()
        for img in all_images:
            if img['url'] not in seen_urls:
                unique_images.append(img)
                seen_urls.add(img['url'])
        
        random.shuffle(unique_images)
        
        print(f"  ğŸ“‹ æ‰¾åˆ° {len(unique_images)} å¼ å»é‡åçš„å›¾ç‰‡")
        
        # ä¸‹è½½å’ŒéªŒè¯å›¾ç‰‡
        downloaded_count = 0
        for i, image_info in enumerate(tqdm(unique_images, desc=f"ä¸‹è½½ {class_name}")):
            if downloaded_count >= target_count:
                break
            
            filename = f"{class_name}_web_{downloaded_count + 1:03d}.jpg"
            filepath = os.path.join(class_dir, filename)
            
            success, message = self.download_and_validate_image(image_info, filepath)
            
            if success:
                downloaded_count += 1
                print(f"    âœ… {filename} - {message}")
            else:
                print(f"    âŒ è·³è¿‡: {message}")
            
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°IP
            time.sleep(random.uniform(1, 4))
        
        print(f"  ğŸ“Š æˆåŠŸæ”¶é›†: {downloaded_count}/{target_count} å¼ å›¾ç‰‡")
        return downloaded_count
    
    def collect_complete_dataset(self):
        """æ”¶é›†å®Œæ•´çš„æ•°æ®é›†"""
        print("ğŸŒ é«˜çº§äººçœ¼å›¾ç‰‡çˆ¬è™«")
        print("=" * 60)
        print("ğŸ¯ ç›®æ ‡: æ”¶é›†100å¼ çœŸå®äººçœ¼å›¾ç‰‡ (25:25:50)")
        print("âœ… åŒ…å«: æœè¯çœ¼éƒ¨ã€é¥®é…’çœ¼éƒ¨ã€æ­£å¸¸çœ¼éƒ¨")
        print("âŒ æ’é™¤: åŠ¨ç‰©ã€æ˜†è™«ã€æ¤ç‰©ã€å¤´åƒã€äººåƒã€å¡é€š")
        
        # ç›®æ ‡åˆ†å¸ƒ
        target_distribution = {
            'stoner': 25,   # æœè¯ï¼ˆå¤§éº»ç­‰ï¼‰
            'alcohol': 25,  # é¥®é…’
            'normal': 50    # æ­£å¸¸
        }
        
        results = {}
        
        for class_name, target_count in target_distribution.items():
            collected = self.collect_images_for_class(class_name, target_count)
            results[class_name] = collected
        
        total_collected = sum(results.values())
        total_target = sum(target_distribution.values())
        
        print(f"\nğŸ‰ æ”¶é›†å®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š ç»“æœ:")
        for class_name, count in results.items():
            target = target_distribution[class_name]
            percentage = (count / target * 100) if target > 0 else 0
            print(f"  {class_name:8s}: {count:2d}/{target} ({percentage:.1f}%)")
        
        print(f"ğŸ“Š æ€»è®¡: {total_collected}/{total_target} å¼ çœŸå®äººçœ¼å›¾ç‰‡")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‘ï¸  é«˜çº§äººçœ¼å›¾ç‰‡çˆ¬è™«")
    print("ğŸ¯ ä¸“é—¨æ”¶é›†çœŸå®çš„äººçœ¼ç…§ç‰‡")
    print("=" * 60)
    
    scraper = AdvancedEyeScraper()
    results = scraper.collect_complete_dataset()
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. éªŒè¯å›¾ç‰‡è´¨é‡: python simple_validator.py")
    print(f"  2. é‡æ–°è®­ç»ƒæ¨¡å‹: python simple_ml_trainer.py")
    print(f"  3. æµ‹è¯•æ¨¡å‹æ€§èƒ½: python simple_inference.py --test")
    print(f"  4. æ¨é€åˆ°GitHub: git add . && git commit && git push")
    
    return results

if __name__ == "__main__":
    results = main()
