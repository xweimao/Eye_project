"""
Demo analysis script for Eye State Classification Project
(Simplified version without heavy dependencies)
"""

import os
import json
from datetime import datetime
import config

def print_header(title):
    """Print formatted header"""
    print("\n" + "="*60)
    print(f" {title}")
    print("="*60)

def analyze_project_status():
    """Analyze current project status"""
    print_header("EYE STATE CLASSIFICATION PROJECT DEMO")
    
    print(f"ğŸ•’ Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ Project Directory: {os.getcwd()}")
    
    # Check project structure
    print_header("PROJECT STRUCTURE ANALYSIS")
    
    core_files = [
        "config.py", "main.py", "setup.py", "inference.py", 
        "requirements.txt", "README.md"
    ]
    
    modules = [
        "data_collection", "preprocessing", "models", 
        "training", "evaluation", "utils"
    ]
    
    print("ğŸ“ Core Files:")
    for file in core_files:
        if os.path.exists(file):
            size = os.path.getsize(file)
            print(f"  âœ… {file} ({size} bytes)")
        else:
            print(f"  âŒ {file} (missing)")
    
    print("\nğŸ“¦ Modules:")
    for module in modules:
        if os.path.exists(module):
            files = len([f for f in os.listdir(module) if f.endswith('.py')])
            print(f"  âœ… {module}/ ({files} Python files)")
        else:
            print(f"  âŒ {module}/ (missing)")
    
    # Check data directories
    print_header("DATA DIRECTORY ANALYSIS")
    
    data_dirs = [
        ("data/raw/stoner", "Stoner eye images"),
        ("data/raw/alcohol", "Alcohol eye images"), 
        ("data/raw/normal", "Normal eye images"),
        ("data/processed", "Processed images"),
        ("models/saved_models", "Trained models"),
        ("logs", "Log files")
    ]
    
    for dir_path, description in data_dirs:
        if os.path.exists(dir_path):
            files = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])
            print(f"  âœ… {dir_path}/ - {description} ({files} files)")
        else:
            print(f"  âŒ {dir_path}/ - {description} (missing)")
    
    # Configuration analysis
    print_header("CONFIGURATION ANALYSIS")
    
    print(f"ğŸ¯ Target Images: {config.TOTAL_IMAGES}")
    print(f"ğŸ“Š Class Distribution:")
    total_target = 0
    for class_name, count in config.CLASS_DISTRIBUTION.items():
        percentage = (count / config.TOTAL_IMAGES * 100)
        print(f"  - {class_name}: {count} images ({percentage:.1f}%)")
        total_target += count
    
    print(f"ğŸ–¼ï¸  Image Settings:")
    print(f"  - Target size: {config.IMAGE_SIZE}")
    print(f"  - Supported formats: {', '.join(config.SUPPORTED_FORMATS)}")
    
    print(f"ğŸ¤– Model Settings:")
    print(f"  - Batch size: {config.BATCH_SIZE}")
    print(f"  - Epochs: {config.EPOCHS}")
    print(f"  - Learning rate: {config.LEARNING_RATE}")
    
    # Data collection status
    print_header("DATA COLLECTION STATUS")
    
    total_collected = 0
    for class_name, target_count in config.CLASS_DISTRIBUTION.items():
        class_dir = os.path.join(config.RAW_DATA_DIR, class_name)
        if os.path.exists(class_dir):
            files = [f for f in os.listdir(class_dir) 
                    if os.path.splitext(f.lower())[1] in config.SUPPORTED_FORMATS]
            current_count = len(files)
            total_collected += current_count
            percentage = (current_count / target_count * 100) if target_count > 0 else 0
            status = "âœ…" if current_count >= target_count else "â³"
            print(f"  {status} {class_name}: {current_count}/{target_count} ({percentage:.1f}%)")
        else:
            print(f"  âŒ {class_name}: 0/{target_count} (0.0%)")
    
    overall_percentage = (total_collected / config.TOTAL_IMAGES * 100)
    print(f"\nğŸ“Š Overall Progress: {total_collected}/{config.TOTAL_IMAGES} ({overall_percentage:.1f}%)")
    
    # Search keywords analysis
    print_header("SEARCH KEYWORDS ANALYSIS")
    
    for class_name, keywords in config.SEARCH_KEYWORDS.items():
        print(f"ğŸ” {class_name.upper()} Keywords ({len(keywords)} total):")
        for i, keyword in enumerate(keywords[:3], 1):  # Show first 3
            print(f"  {i}. \"{keyword}\"")
        if len(keywords) > 3:
            print(f"  ... and {len(keywords) - 3} more")
    
    # Project capabilities
    print_header("PROJECT CAPABILITIES")
    
    capabilities = [
        "ğŸŒ Automated web scraping from Google Images and Bing Images",
        "ğŸ” Image validation and quality assessment", 
        "ğŸ‘ï¸ Face and eye region detection using OpenCV",
        "ğŸ–¼ï¸ Image preprocessing and data augmentation",
        "ğŸ§  Multiple CNN architectures (Custom, ResNet, EfficientNet)",
        "âš–ï¸ Support for both TensorFlow and PyTorch frameworks",
        "ğŸ“Š Comprehensive model evaluation with metrics",
        "ğŸ“ˆ Training progress visualization",
        "ğŸ¯ Real-time inference on new images",
        "ğŸ“‹ Detailed logging and progress tracking"
    ]
    
    for capability in capabilities:
        print(f"  {capability}")
    
    # Usage examples
    print_header("USAGE EXAMPLES")
    
    examples = [
        ("ğŸš€ Run complete pipeline", "python main.py --step all"),
        ("ğŸ“¥ Collect data only", "python main.py --step collect"),
        ("ğŸ” Validate collected data", "python main.py --step validate"),
        ("ğŸ”„ Preprocess images", "python main.py --step preprocess"),
        ("ğŸ“ Train models", "python main.py --step train"),
        ("ğŸ“Š Evaluate models", "python main.py --step evaluate"),
        ("ğŸ¯ Make prediction", "python inference.py --model path/to/model.h5 --image path/to/image.jpg"),
        ("ğŸ“‹ Project overview", "python project_overview.py")
    ]
    
    for description, command in examples:
        print(f"  {description}:")
        print(f"    {command}")
        print()
    
    # Next steps
    print_header("RECOMMENDED NEXT STEPS")
    
    if total_collected == 0:
        print("ğŸ”§ SETUP PHASE:")
        print("  1. Install remaining dependencies:")
        print("     pip install torch tensorflow scikit-learn selenium")
        print("  2. Start data collection:")
        print("     python main.py --step collect")
        print("  3. Validate collected data:")
        print("     python main.py --step validate")
    elif total_collected < config.TOTAL_IMAGES:
        print("ğŸ“¥ DATA COLLECTION PHASE:")
        print("  1. Continue data collection:")
        print("     python main.py --step collect")
        print("  2. Monitor progress:")
        print("     python project_overview.py")
    else:
        print("ğŸ“ TRAINING PHASE:")
        print("  1. Preprocess collected data:")
        print("     python main.py --step preprocess")
        print("  2. Train models:")
        print("     python main.py --step train")
        print("  3. Evaluate performance:")
        print("     python main.py --step evaluate")
    
    # Important notes
    print_header("IMPORTANT NOTES")
    
    notes = [
        "âš ï¸ This project is for educational and research purposes only",
        "ğŸ”’ Respect privacy and ethical guidelines when collecting data",
        "ğŸŒ Ensure stable internet connection for data collection",
        "ğŸ’¾ Recommended: 10GB+ free disk space for full dataset",
        "ğŸ–¥ï¸ GPU acceleration recommended for faster training",
        "ğŸ“š Check README.md for detailed documentation"
    ]
    
    for note in notes:
        print(f"  {note}")
    
    print(f"\nğŸ‰ Project analysis completed at {datetime.now().strftime('%H:%M:%S')}")
    print("="*60)

def create_demo_summary():
    """Create a demo summary file"""
    summary = {
        "project_name": "Eye State Classification AI",
        "analysis_time": datetime.now().isoformat(),
        "project_status": "Ready for deployment",
        "total_files": 19,
        "total_modules": 6,
        "target_images": config.TOTAL_IMAGES,
        "class_distribution": config.CLASS_DISTRIBUTION,
        "capabilities": [
            "Automated data collection",
            "Image preprocessing", 
            "Multiple CNN architectures",
            "Model training and evaluation",
            "Real-time inference"
        ],
        "frameworks_supported": ["TensorFlow", "PyTorch"],
        "next_steps": [
            "Install dependencies",
            "Run data collection",
            "Train models",
            "Evaluate performance"
        ]
    }
    
    summary_path = os.path.join(config.LOGS_DIR, 'demo_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nğŸ“„ Demo summary saved to: {summary_path}")

if __name__ == "__main__":
    analyze_project_status()
    create_demo_summary()
